{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import trange\n",
    "from torchsummary import summary\n",
    "from torchstat import stat\n",
    "\n",
    "YOUR_PATH = \"/home/jyt/workspace/fNIRS_models/code_data_tufts\"\n",
    "sys.path.insert(0, YOUR_PATH + '/fNIRS-mental-workload-classifiers/helpers')\n",
    "import models\n",
    "import brain_data\n",
    "from utils import generic_GetTrainValTestSubjects, seed_everything, makedir_if_not_exist, plot_confusion_matrix, save_pickle, train_one_epoch, eval_model, save_training_curves_FixedTrainValSplit, write_performance_info_FixedTrainValSplit, write_program_time, write_inference_time\n",
    "from utils import LabelSmoothing, train_one_epoch_fNIRS_T, eval_model_fNIRST, train_one_epoch_Ours_T, eval_model_OursT\n",
    "from utils import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OursT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPUs\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('Detected GPUs', flush = True)\n",
    "    #device = torch.device('cuda')\n",
    "    device = torch.device('cuda:{}'.format(0))\n",
    "else:\n",
    "    print('DID NOT detect GPUs', flush = True)\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = models.Ours_T\n",
    "model = model_to_use(n_class=2, sampling_points=150, patch_length=30, dim=64, depth=6, heads=8, mlp_dim=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 8, 150]               0\n",
      "         Rearrange-2             [-1, 8, 5, 30]               0\n",
      "         Rearrange-3             [-1, 5, 8, 30]               0\n",
      "         Rearrange-4               [-1, 5, 240]               0\n",
      "            Linear-5                [-1, 5, 64]          15,424\n",
      "           Dropout-6                [-1, 6, 64]               0\n",
      "         LayerNorm-7                [-1, 6, 64]             128\n",
      "            Linear-8              [-1, 6, 1536]          98,304\n",
      "            Linear-9                [-1, 6, 64]          32,832\n",
      "          Dropout-10                [-1, 6, 64]               0\n",
      "        Attention-11                [-1, 6, 64]               0\n",
      "          PreNorm-12                [-1, 6, 64]               0\n",
      "         Residual-13                [-1, 6, 64]               0\n",
      "        LayerNorm-14                [-1, 6, 64]             128\n",
      "           Linear-15               [-1, 6, 256]          16,640\n",
      "             GELU-16               [-1, 6, 256]               0\n",
      "          Dropout-17               [-1, 6, 256]               0\n",
      "           Linear-18                [-1, 6, 64]          16,448\n",
      "          Dropout-19                [-1, 6, 64]               0\n",
      "      FeedForward-20                [-1, 6, 64]               0\n",
      "          PreNorm-21                [-1, 6, 64]               0\n",
      "         Residual-22                [-1, 6, 64]               0\n",
      "        LayerNorm-23                [-1, 6, 64]             128\n",
      "           Linear-24              [-1, 6, 1536]          98,304\n",
      "           Linear-25                [-1, 6, 64]          32,832\n",
      "          Dropout-26                [-1, 6, 64]               0\n",
      "        Attention-27                [-1, 6, 64]               0\n",
      "          PreNorm-28                [-1, 6, 64]               0\n",
      "         Residual-29                [-1, 6, 64]               0\n",
      "        LayerNorm-30                [-1, 6, 64]             128\n",
      "           Linear-31               [-1, 6, 256]          16,640\n",
      "             GELU-32               [-1, 6, 256]               0\n",
      "          Dropout-33               [-1, 6, 256]               0\n",
      "           Linear-34                [-1, 6, 64]          16,448\n",
      "          Dropout-35                [-1, 6, 64]               0\n",
      "      FeedForward-36                [-1, 6, 64]               0\n",
      "          PreNorm-37                [-1, 6, 64]               0\n",
      "         Residual-38                [-1, 6, 64]               0\n",
      "        LayerNorm-39                [-1, 6, 64]             128\n",
      "           Linear-40              [-1, 6, 1536]          98,304\n",
      "           Linear-41                [-1, 6, 64]          32,832\n",
      "          Dropout-42                [-1, 6, 64]               0\n",
      "        Attention-43                [-1, 6, 64]               0\n",
      "          PreNorm-44                [-1, 6, 64]               0\n",
      "         Residual-45                [-1, 6, 64]               0\n",
      "        LayerNorm-46                [-1, 6, 64]             128\n",
      "           Linear-47               [-1, 6, 256]          16,640\n",
      "             GELU-48               [-1, 6, 256]               0\n",
      "          Dropout-49               [-1, 6, 256]               0\n",
      "           Linear-50                [-1, 6, 64]          16,448\n",
      "          Dropout-51                [-1, 6, 64]               0\n",
      "      FeedForward-52                [-1, 6, 64]               0\n",
      "          PreNorm-53                [-1, 6, 64]               0\n",
      "         Residual-54                [-1, 6, 64]               0\n",
      "        LayerNorm-55                [-1, 6, 64]             128\n",
      "           Linear-56              [-1, 6, 1536]          98,304\n",
      "           Linear-57                [-1, 6, 64]          32,832\n",
      "          Dropout-58                [-1, 6, 64]               0\n",
      "        Attention-59                [-1, 6, 64]               0\n",
      "          PreNorm-60                [-1, 6, 64]               0\n",
      "         Residual-61                [-1, 6, 64]               0\n",
      "        LayerNorm-62                [-1, 6, 64]             128\n",
      "           Linear-63               [-1, 6, 256]          16,640\n",
      "             GELU-64               [-1, 6, 256]               0\n",
      "          Dropout-65               [-1, 6, 256]               0\n",
      "           Linear-66                [-1, 6, 64]          16,448\n",
      "          Dropout-67                [-1, 6, 64]               0\n",
      "      FeedForward-68                [-1, 6, 64]               0\n",
      "          PreNorm-69                [-1, 6, 64]               0\n",
      "         Residual-70                [-1, 6, 64]               0\n",
      "        LayerNorm-71                [-1, 6, 64]             128\n",
      "           Linear-72              [-1, 6, 1536]          98,304\n",
      "           Linear-73                [-1, 6, 64]          32,832\n",
      "          Dropout-74                [-1, 6, 64]               0\n",
      "        Attention-75                [-1, 6, 64]               0\n",
      "          PreNorm-76                [-1, 6, 64]               0\n",
      "         Residual-77                [-1, 6, 64]               0\n",
      "        LayerNorm-78                [-1, 6, 64]             128\n",
      "           Linear-79               [-1, 6, 256]          16,640\n",
      "             GELU-80               [-1, 6, 256]               0\n",
      "          Dropout-81               [-1, 6, 256]               0\n",
      "           Linear-82                [-1, 6, 64]          16,448\n",
      "          Dropout-83                [-1, 6, 64]               0\n",
      "      FeedForward-84                [-1, 6, 64]               0\n",
      "          PreNorm-85                [-1, 6, 64]               0\n",
      "         Residual-86                [-1, 6, 64]               0\n",
      "        LayerNorm-87                [-1, 6, 64]             128\n",
      "           Linear-88              [-1, 6, 1536]          98,304\n",
      "           Linear-89                [-1, 6, 64]          32,832\n",
      "          Dropout-90                [-1, 6, 64]               0\n",
      "        Attention-91                [-1, 6, 64]               0\n",
      "          PreNorm-92                [-1, 6, 64]               0\n",
      "         Residual-93                [-1, 6, 64]               0\n",
      "        LayerNorm-94                [-1, 6, 64]             128\n",
      "           Linear-95               [-1, 6, 256]          16,640\n",
      "             GELU-96               [-1, 6, 256]               0\n",
      "          Dropout-97               [-1, 6, 256]               0\n",
      "           Linear-98                [-1, 6, 64]          16,448\n",
      "          Dropout-99                [-1, 6, 64]               0\n",
      "     FeedForward-100                [-1, 6, 64]               0\n",
      "         PreNorm-101                [-1, 6, 64]               0\n",
      "        Residual-102                [-1, 6, 64]               0\n",
      "     Transformer-103                [-1, 6, 64]               0\n",
      "       LayerNorm-104                   [-1, 64]             128\n",
      "          Linear-105                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 1,002,562\n",
      "Trainable params: 1,002,562\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.89\n",
      "Params size (MB): 3.82\n",
      "Estimated Total Size (MB): 4.72\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(150, 8), batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = models.fNIRS_PreT\n",
    "model = model_to_use(n_class=2, sampling_point=300, dim=64, depth=6, heads=8, mlp_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AvgPool1d-1               [-1, 2, 300]               0\n",
      "         AvgPool1d-2               [-1, 2, 300]               0\n",
      "         AvgPool1d-3               [-1, 2, 300]               0\n",
      "         LayerNorm-4               [-1, 2, 300]             600\n",
      "         AvgPool1d-5               [-1, 2, 300]               0\n",
      "         AvgPool1d-6               [-1, 2, 300]               0\n",
      "         AvgPool1d-7               [-1, 2, 300]               0\n",
      "         LayerNorm-8               [-1, 2, 300]             600\n",
      "          PreBlock-9            [-1, 2, 2, 300]               0\n",
      "           Conv2d-10             [-1, 8, 3, 91]             968\n",
      "        Rearrange-11               [-1, 3, 728]               0\n",
      "           Linear-12                [-1, 3, 64]          46,656\n",
      "        LayerNorm-13                [-1, 3, 64]             128\n",
      "           Conv2d-14             [-1, 8, 2, 91]             488\n",
      "        Rearrange-15               [-1, 2, 728]               0\n",
      "           Linear-16                [-1, 2, 64]          46,656\n",
      "        LayerNorm-17                [-1, 2, 64]             128\n",
      "          Dropout-18                [-1, 4, 64]               0\n",
      "        LayerNorm-19                [-1, 4, 64]             128\n",
      "           Linear-20              [-1, 4, 1536]          98,304\n",
      "           Linear-21                [-1, 4, 64]          32,832\n",
      "          Dropout-22                [-1, 4, 64]               0\n",
      "        Attention-23                [-1, 4, 64]               0\n",
      "          PreNorm-24                [-1, 4, 64]               0\n",
      "         Residual-25                [-1, 4, 64]               0\n",
      "        LayerNorm-26                [-1, 4, 64]             128\n",
      "           Linear-27                [-1, 4, 64]           4,160\n",
      "             GELU-28                [-1, 4, 64]               0\n",
      "          Dropout-29                [-1, 4, 64]               0\n",
      "           Linear-30                [-1, 4, 64]           4,160\n",
      "          Dropout-31                [-1, 4, 64]               0\n",
      "      FeedForward-32                [-1, 4, 64]               0\n",
      "          PreNorm-33                [-1, 4, 64]               0\n",
      "         Residual-34                [-1, 4, 64]               0\n",
      "        LayerNorm-35                [-1, 4, 64]             128\n",
      "           Linear-36              [-1, 4, 1536]          98,304\n",
      "           Linear-37                [-1, 4, 64]          32,832\n",
      "          Dropout-38                [-1, 4, 64]               0\n",
      "        Attention-39                [-1, 4, 64]               0\n",
      "          PreNorm-40                [-1, 4, 64]               0\n",
      "         Residual-41                [-1, 4, 64]               0\n",
      "        LayerNorm-42                [-1, 4, 64]             128\n",
      "           Linear-43                [-1, 4, 64]           4,160\n",
      "             GELU-44                [-1, 4, 64]               0\n",
      "          Dropout-45                [-1, 4, 64]               0\n",
      "           Linear-46                [-1, 4, 64]           4,160\n",
      "          Dropout-47                [-1, 4, 64]               0\n",
      "      FeedForward-48                [-1, 4, 64]               0\n",
      "          PreNorm-49                [-1, 4, 64]               0\n",
      "         Residual-50                [-1, 4, 64]               0\n",
      "        LayerNorm-51                [-1, 4, 64]             128\n",
      "           Linear-52              [-1, 4, 1536]          98,304\n",
      "           Linear-53                [-1, 4, 64]          32,832\n",
      "          Dropout-54                [-1, 4, 64]               0\n",
      "        Attention-55                [-1, 4, 64]               0\n",
      "          PreNorm-56                [-1, 4, 64]               0\n",
      "         Residual-57                [-1, 4, 64]               0\n",
      "        LayerNorm-58                [-1, 4, 64]             128\n",
      "           Linear-59                [-1, 4, 64]           4,160\n",
      "             GELU-60                [-1, 4, 64]               0\n",
      "          Dropout-61                [-1, 4, 64]               0\n",
      "           Linear-62                [-1, 4, 64]           4,160\n",
      "          Dropout-63                [-1, 4, 64]               0\n",
      "      FeedForward-64                [-1, 4, 64]               0\n",
      "          PreNorm-65                [-1, 4, 64]               0\n",
      "         Residual-66                [-1, 4, 64]               0\n",
      "        LayerNorm-67                [-1, 4, 64]             128\n",
      "           Linear-68              [-1, 4, 1536]          98,304\n",
      "           Linear-69                [-1, 4, 64]          32,832\n",
      "          Dropout-70                [-1, 4, 64]               0\n",
      "        Attention-71                [-1, 4, 64]               0\n",
      "          PreNorm-72                [-1, 4, 64]               0\n",
      "         Residual-73                [-1, 4, 64]               0\n",
      "        LayerNorm-74                [-1, 4, 64]             128\n",
      "           Linear-75                [-1, 4, 64]           4,160\n",
      "             GELU-76                [-1, 4, 64]               0\n",
      "          Dropout-77                [-1, 4, 64]               0\n",
      "           Linear-78                [-1, 4, 64]           4,160\n",
      "          Dropout-79                [-1, 4, 64]               0\n",
      "      FeedForward-80                [-1, 4, 64]               0\n",
      "          PreNorm-81                [-1, 4, 64]               0\n",
      "         Residual-82                [-1, 4, 64]               0\n",
      "        LayerNorm-83                [-1, 4, 64]             128\n",
      "           Linear-84              [-1, 4, 1536]          98,304\n",
      "           Linear-85                [-1, 4, 64]          32,832\n",
      "          Dropout-86                [-1, 4, 64]               0\n",
      "        Attention-87                [-1, 4, 64]               0\n",
      "          PreNorm-88                [-1, 4, 64]               0\n",
      "         Residual-89                [-1, 4, 64]               0\n",
      "        LayerNorm-90                [-1, 4, 64]             128\n",
      "           Linear-91                [-1, 4, 64]           4,160\n",
      "             GELU-92                [-1, 4, 64]               0\n",
      "          Dropout-93                [-1, 4, 64]               0\n",
      "           Linear-94                [-1, 4, 64]           4,160\n",
      "          Dropout-95                [-1, 4, 64]               0\n",
      "      FeedForward-96                [-1, 4, 64]               0\n",
      "          PreNorm-97                [-1, 4, 64]               0\n",
      "         Residual-98                [-1, 4, 64]               0\n",
      "        LayerNorm-99                [-1, 4, 64]             128\n",
      "          Linear-100              [-1, 4, 1536]          98,304\n",
      "          Linear-101                [-1, 4, 64]          32,832\n",
      "         Dropout-102                [-1, 4, 64]               0\n",
      "       Attention-103                [-1, 4, 64]               0\n",
      "         PreNorm-104                [-1, 4, 64]               0\n",
      "        Residual-105                [-1, 4, 64]               0\n",
      "       LayerNorm-106                [-1, 4, 64]             128\n",
      "          Linear-107                [-1, 4, 64]           4,160\n",
      "            GELU-108                [-1, 4, 64]               0\n",
      "         Dropout-109                [-1, 4, 64]               0\n",
      "          Linear-110                [-1, 4, 64]           4,160\n",
      "         Dropout-111                [-1, 4, 64]               0\n",
      "     FeedForward-112                [-1, 4, 64]               0\n",
      "         PreNorm-113                [-1, 4, 64]               0\n",
      "        Residual-114                [-1, 4, 64]               0\n",
      "     Transformer-115                [-1, 4, 64]               0\n",
      "         Dropout-116                [-1, 3, 64]               0\n",
      "       LayerNorm-117                [-1, 3, 64]             128\n",
      "          Linear-118              [-1, 3, 1536]          98,304\n",
      "          Linear-119                [-1, 3, 64]          32,832\n",
      "         Dropout-120                [-1, 3, 64]               0\n",
      "       Attention-121                [-1, 3, 64]               0\n",
      "         PreNorm-122                [-1, 3, 64]               0\n",
      "        Residual-123                [-1, 3, 64]               0\n",
      "       LayerNorm-124                [-1, 3, 64]             128\n",
      "          Linear-125                [-1, 3, 64]           4,160\n",
      "            GELU-126                [-1, 3, 64]               0\n",
      "         Dropout-127                [-1, 3, 64]               0\n",
      "          Linear-128                [-1, 3, 64]           4,160\n",
      "         Dropout-129                [-1, 3, 64]               0\n",
      "     FeedForward-130                [-1, 3, 64]               0\n",
      "         PreNorm-131                [-1, 3, 64]               0\n",
      "        Residual-132                [-1, 3, 64]               0\n",
      "       LayerNorm-133                [-1, 3, 64]             128\n",
      "          Linear-134              [-1, 3, 1536]          98,304\n",
      "          Linear-135                [-1, 3, 64]          32,832\n",
      "         Dropout-136                [-1, 3, 64]               0\n",
      "       Attention-137                [-1, 3, 64]               0\n",
      "         PreNorm-138                [-1, 3, 64]               0\n",
      "        Residual-139                [-1, 3, 64]               0\n",
      "       LayerNorm-140                [-1, 3, 64]             128\n",
      "          Linear-141                [-1, 3, 64]           4,160\n",
      "            GELU-142                [-1, 3, 64]               0\n",
      "         Dropout-143                [-1, 3, 64]               0\n",
      "          Linear-144                [-1, 3, 64]           4,160\n",
      "         Dropout-145                [-1, 3, 64]               0\n",
      "     FeedForward-146                [-1, 3, 64]               0\n",
      "         PreNorm-147                [-1, 3, 64]               0\n",
      "        Residual-148                [-1, 3, 64]               0\n",
      "       LayerNorm-149                [-1, 3, 64]             128\n",
      "          Linear-150              [-1, 3, 1536]          98,304\n",
      "          Linear-151                [-1, 3, 64]          32,832\n",
      "         Dropout-152                [-1, 3, 64]               0\n",
      "       Attention-153                [-1, 3, 64]               0\n",
      "         PreNorm-154                [-1, 3, 64]               0\n",
      "        Residual-155                [-1, 3, 64]               0\n",
      "       LayerNorm-156                [-1, 3, 64]             128\n",
      "          Linear-157                [-1, 3, 64]           4,160\n",
      "            GELU-158                [-1, 3, 64]               0\n",
      "         Dropout-159                [-1, 3, 64]               0\n",
      "          Linear-160                [-1, 3, 64]           4,160\n",
      "         Dropout-161                [-1, 3, 64]               0\n",
      "     FeedForward-162                [-1, 3, 64]               0\n",
      "         PreNorm-163                [-1, 3, 64]               0\n",
      "        Residual-164                [-1, 3, 64]               0\n",
      "       LayerNorm-165                [-1, 3, 64]             128\n",
      "          Linear-166              [-1, 3, 1536]          98,304\n",
      "          Linear-167                [-1, 3, 64]          32,832\n",
      "         Dropout-168                [-1, 3, 64]               0\n",
      "       Attention-169                [-1, 3, 64]               0\n",
      "         PreNorm-170                [-1, 3, 64]               0\n",
      "        Residual-171                [-1, 3, 64]               0\n",
      "       LayerNorm-172                [-1, 3, 64]             128\n",
      "          Linear-173                [-1, 3, 64]           4,160\n",
      "            GELU-174                [-1, 3, 64]               0\n",
      "         Dropout-175                [-1, 3, 64]               0\n",
      "          Linear-176                [-1, 3, 64]           4,160\n",
      "         Dropout-177                [-1, 3, 64]               0\n",
      "     FeedForward-178                [-1, 3, 64]               0\n",
      "         PreNorm-179                [-1, 3, 64]               0\n",
      "        Residual-180                [-1, 3, 64]               0\n",
      "       LayerNorm-181                [-1, 3, 64]             128\n",
      "          Linear-182              [-1, 3, 1536]          98,304\n",
      "          Linear-183                [-1, 3, 64]          32,832\n",
      "         Dropout-184                [-1, 3, 64]               0\n",
      "       Attention-185                [-1, 3, 64]               0\n",
      "         PreNorm-186                [-1, 3, 64]               0\n",
      "        Residual-187                [-1, 3, 64]               0\n",
      "       LayerNorm-188                [-1, 3, 64]             128\n",
      "          Linear-189                [-1, 3, 64]           4,160\n",
      "            GELU-190                [-1, 3, 64]               0\n",
      "         Dropout-191                [-1, 3, 64]               0\n",
      "          Linear-192                [-1, 3, 64]           4,160\n",
      "         Dropout-193                [-1, 3, 64]               0\n",
      "     FeedForward-194                [-1, 3, 64]               0\n",
      "         PreNorm-195                [-1, 3, 64]               0\n",
      "        Residual-196                [-1, 3, 64]               0\n",
      "       LayerNorm-197                [-1, 3, 64]             128\n",
      "          Linear-198              [-1, 3, 1536]          98,304\n",
      "          Linear-199                [-1, 3, 64]          32,832\n",
      "         Dropout-200                [-1, 3, 64]               0\n",
      "       Attention-201                [-1, 3, 64]               0\n",
      "         PreNorm-202                [-1, 3, 64]               0\n",
      "        Residual-203                [-1, 3, 64]               0\n",
      "       LayerNorm-204                [-1, 3, 64]             128\n",
      "          Linear-205                [-1, 3, 64]           4,160\n",
      "            GELU-206                [-1, 3, 64]               0\n",
      "         Dropout-207                [-1, 3, 64]               0\n",
      "          Linear-208                [-1, 3, 64]           4,160\n",
      "         Dropout-209                [-1, 3, 64]               0\n",
      "     FeedForward-210                [-1, 3, 64]               0\n",
      "         PreNorm-211                [-1, 3, 64]               0\n",
      "        Residual-212                [-1, 3, 64]               0\n",
      "     Transformer-213                [-1, 3, 64]               0\n",
      "        Identity-214                   [-1, 64]               0\n",
      "        Identity-215                   [-1, 64]               0\n",
      "       LayerNorm-216                  [-1, 128]             256\n",
      "          Linear-217                    [-1, 2]             258\n",
      "         fNIRS_T-218                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 1,773,282\n",
      "Trainable params: 1,773,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.91\n",
      "Params size (MB): 6.76\n",
      "Estimated Total Size (MB): 7.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(2,2,300), batch_size=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fNIRS-mental-workload-classifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d93f6ccf2d4c808a6e5ed058fec4d677fc443ecf11a44bde689b875892dca5c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
